{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time Series Classification Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TSML\n",
    "using TSMLextra\n",
    "using DataFrames\n",
    "using Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's add workers for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()==1 && addprocs()\n",
    "nworkers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load TSML Modules and other Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere using TSML\n",
    "@everywhere using TSMLextra\n",
    "ENV[\"COLUMNS\"]=1000; # for dataframe column size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere function predict(learner,data,train_ind,test_ind)\n",
    "    features = convert(Matrix,data[:, 1:(end-1)])\n",
    "    labels = convert(Array,data[:, end])\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(\n",
    "       Dict(\n",
    "         :transformers => [\n",
    "           OneHotEncoder(), # Encodes nominal features into numeric\n",
    "           Imputer(), # Imputes NA values\n",
    "           StandardScaler(),\n",
    "           learner # Predicts labels on instances\n",
    "         ]\n",
    "       )\n",
    "    )\n",
    "    # Train\n",
    "    fit!(pipeline, features[train_ind, :], labels[train_ind]);\n",
    "    # Predict\n",
    "    predictions = transform!(pipeline, features[test_ind, :]);\n",
    "    # Assess predictions\n",
    "    result = score(:accuracy, labels[test_ind], predictions)\n",
    "    return result,pipeline\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run in parallel all models in different trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parallelmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function parallelmodel(learners::Dict,data::DataFrame;trials=5)\n",
    "    models=collect(keys(learners))\n",
    "    ctable=@distributed (vcat) for i=1:trials\n",
    "        # Split into training and test sets\n",
    "        Random.seed!(rand(1:100,1)[1])\n",
    "        (train_ind, test_ind) = holdout(size(data, 1), 0.20)\n",
    "        acc=@distributed (vcat) for model in models\n",
    "            res,_=predict(learners[model],data,train_ind,test_ind)\n",
    "            println(\"trial \",i,\", \",model,\" => \",round(res))\n",
    "            [model res i]\n",
    "        end\n",
    "        acc\n",
    "    end\n",
    "    df = ctable |> DataFrame\n",
    "    rename!(df,:x1=>:model,:x2=>:acc,:x3=>:trial)\n",
    "    gp=by(df,:model) do x\n",
    "       DataFrame(mean=mean(x.acc),std=std(x.acc),n=length(x.acc)) \n",
    "    end\n",
    "    sort!(gp,:mean,rev=true)\n",
    "    return gp\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize ML models from Julia, Caret, and Scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Caret ML\n",
    "@everywhere caret_svmlinear = CaretLearner(Dict(:learner=>\"svmLinear\"))\n",
    "@everywhere caret_treebag = CaretLearner(Dict(:learner=>\"treebag\"))\n",
    "@everywhere caret_rpart = CaretLearner(Dict(:learner=>\"rpart\"))\n",
    "@everywhere caret_rf = CaretLearner(Dict(:learner=>\"rf\"))\n",
    "\n",
    "# ScikitLearn ML\n",
    "@everywhere sk_ridge = SKLearner(Dict(:learner=>\"RidgeClassifier\"))\n",
    "@everywhere sk_sgd = SKLearner(Dict(:learner=>\"SGDClassifier\"))\n",
    "@everywhere sk_knn = SKLearner(Dict(:learner=>\"KNeighborsClassifier\"))\n",
    "@everywhere sk_gb = SKLearner(Dict(:learner=>\"GradientBoostingClassifier\",:impl_args=>Dict(:n_estimators=>10)))\n",
    "@everywhere sk_extratree = SKLearner(Dict(:learner=>\"ExtraTreesClassifier\",:impl_args=>Dict(:n_estimators=>10)))\n",
    "@everywhere sk_rf = SKLearner(Dict(:learner=>\"RandomForestClassifier\",:impl_args=>Dict(:n_estimators=>10)))\n",
    "\n",
    "# Julia ML\n",
    "@everywhere jrf = RandomForest(Dict(:impl_args=>Dict(:num_trees=>300)))\n",
    "@everywhere jpt = PrunedTree()\n",
    "@everywhere jada = Adaboost()\n",
    "\n",
    "# Julia Ensembles\n",
    "@everywhere jvote_ens=VoteEnsemble(Dict(:learners=>[jrf,jpt,sk_gb,sk_extratree,caret_rf]))\n",
    "@everywhere jstack_ada=StackEnsemble(Dict(:stacker=>Adaboost(),:learners=>[jrf,jpt,sk_gb,sk_extratree,caret_rf]))\n",
    "@everywhere jstack_rf=StackEnsemble(Dict(:stacker=>RandomForest(),:learners=>[jrf,jpt,sk_gb,sk_extratree,caret_rf]))\n",
    "@everywhere jbest_ens=BestLearner(Dict(:learners=>[jrf,sk_gb,caret_rf]))\n",
    "@everywhere jsuper_ens=VoteEnsemble(Dict(:learners=>[jvote_ens,jstack_ada,jstack_rf,sk_gb,caret_rf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use iris dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Sepal_Length</th><th>Sepal_Width</th><th>Petal_Length</th><th>Petal_Width</th><th>Species</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Categorical…</th></tr></thead><tbody><p>5 rows × 5 columns</p><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Sepal\\_Length & Sepal\\_Width & Petal\\_Length & Petal\\_Width & Species\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Categorical…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa \\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×5 DataFrame\n",
       "│ Row │ Sepal_Length │ Sepal_Width │ Petal_Length │ Petal_Width │ Species      │\n",
       "│     │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mCategorical…\u001b[39m │\n",
       "├─────┼──────────────┼─────────────┼──────────────┼─────────────┼──────────────┤\n",
       "│ 1   │ 5.1          │ 3.5         │ 1.4          │ 0.2         │ setosa       │\n",
       "│ 2   │ 4.9          │ 3.0         │ 1.4          │ 0.2         │ setosa       │\n",
       "│ 3   │ 4.7          │ 3.2         │ 1.3          │ 0.2         │ setosa       │\n",
       "│ 4   │ 4.6          │ 3.1         │ 1.5          │ 0.2         │ setosa       │\n",
       "│ 5   │ 5.0          │ 3.6         │ 1.4          │ 0.2         │ setosa       │"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RCall\n",
    "iris = R\"iris\"|> rcopy\n",
    "first(iris,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in parallel different learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "learners=Dict(\n",
    "      :jvote_ens=>jvote_ens,:jstack_rf=>jstack_rf,:jbest_ens=>jbest_ens, :jstack_ada=>jstack_ada,\n",
    "      :jrf => jrf,:jada=>jada,:jsuper_ens=>jsuper_ens,:crt_rpart=>caret_rpart,\n",
    "      :crt_svmlinear=>caret_svmlinear,:crt_treebag=>caret_treebag,:crt_rf=>caret_rf, \n",
    "      :skl_knn=>sk_knn,:skl_gb=>sk_gb,:skl_extratree=>sk_extratree,\n",
    "      :sk_rf => sk_rf\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\tWARNING: redefining constant python\n",
      "      From worker 8:\tWARNING: redefining constant python\n",
      "      From worker 5:\tWARNING: redefining constant python\n",
      "      From worker 2:\tWARNING: redefining constant python\n",
      "      From worker 6:\tWARNING: redefining constant python\n",
      "      From worker 4:\tWARNING: redefining constant python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant python\n",
      "WARNING: redefining constant libpython\n",
      "WARNING: redefining constant pyprogramname\n",
      "WARNING: redefining constant pyversion_build\n",
      "WARNING: redefining constant PYTHONHOME\n",
      "WARNING: redefining constant conda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 7:\tWARNING: redefining constant python\n",
      "      From worker 7:\tWARNING: redefining constant libpython\n",
      "      From worker 7:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 7:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 7:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 7:\tWARNING: redefining constant conda\n",
      "      From worker 9:\tWARNING: redefining constant python\n",
      "      From worker 9:\tWARNING: redefining constant libpython\n",
      "      From worker 9:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 9:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 9:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 9:\tWARNING: redefining constant conda\n",
      "      From worker 4:\tWARNING: redefining constant libpython\n",
      "      From worker 4:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 4:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 4:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 4:\tWARNING: redefining constant conda\n",
      "      From worker 3:\tWARNING: redefining constant libpython\n",
      "      From worker 3:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 3:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 3:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 3:\tWARNING: redefining constant conda\n",
      "      From worker 5:\tWARNING: redefining constant libpython\n",
      "      From worker 5:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 5:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 5:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 5:\tWARNING: redefining constant conda\n",
      "      From worker 8:\tWARNING: redefining constant libpython\n",
      "      From worker 8:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 8:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 8:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 8:\tWARNING: redefining constant conda\n",
      "      From worker 6:\tWARNING: redefining constant libpython\n",
      "      From worker 6:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 6:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 6:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 6:\tWARNING: redefining constant conda\n",
      "      From worker 2:\tWARNING: redefining constant libpython\n",
      "      From worker 2:\tWARNING: redefining constant pyprogramname\n",
      "      From worker 2:\tWARNING: redefining constant pyversion_build\n",
      "      From worker 2:\tWARNING: redefining constant PYTHONHOME\n",
      "      From worker 2:\tWARNING: redefining constant conda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n",
      "┌ Warning: Replacing docs for `PyCall.conda :: Union{}` in module `PyCall`\n",
      "└ @ Base.Docs docs/Docs.jl:223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\ttrial 2, jstack_ada => 93.0\n",
      "      From worker 7:\ttrial 2, jada => 93.0\n",
      "      From worker 9:\ttrial 2, skl_knn => 90.0\n",
      "      From worker 9:\ttrial 3, skl_knn => 97.0\n",
      "      From worker 9:\ttrial 1, skl_knn => 97.0\n",
      "      From worker 2:\ttrial 1, jstack_ada => 97.0\n",
      "      From worker 3:\ttrial 2, jrf => 93.0\n",
      "      From worker 7:\ttrial 2, crt_svmlinear => 93.0\n",
      "      From worker 7:\ttrial 3, jada => 100.0\n",
      "      From worker 4:\ttrial 3, jstack_ada => 97.0\n",
      "      From worker 4:\ttrial 2, crt_rf => 93.0\n",
      "      From worker 7:\ttrial 3, crt_svmlinear => 97.0\n",
      "      From worker 7:\ttrial 1, jada => 100.0\n",
      "      From worker 4:\ttrial 2, crt_rpart => 30.0\n",
      "      From worker 4:\ttrial 1, crt_rf => 97.0\n",
      "      From worker 7:\ttrial 1, crt_svmlinear => 100.0\n",
      "      From worker 6:\ttrial 2, jvote_ens => 93.0\n",
      "      From worker 6:\ttrial 2, skl_gb => 93.0\n",
      "      From worker 6:\ttrial 3, jvote_ens => 97.0\n",
      "      From worker 6:\ttrial 3, skl_gb => 100.0\n",
      "      From worker 6:\ttrial 1, jvote_ens => 97.0\n",
      "      From worker 6:\ttrial 1, skl_gb => 97.0\n",
      "      From worker 5:\ttrial 2, crt_treebag => 93.0\n",
      "      From worker 4:\ttrial 1, crt_rpart => 30.0\n",
      "      From worker 4:\ttrial 3, jrf => 100.0\n",
      "      From worker 5:\ttrial 3, crt_treebag => 100.0\n",
      "      From worker 5:\ttrial 1, crt_treebag => 97.0\n",
      "      From worker 3:\ttrial 3, crt_rf => 97.0\n",
      "      From worker 5:\ttrial 2, jstack_rf => 93.0\n",
      "      From worker 5:\ttrial 3, jstack_rf => 97.0\n",
      "      From worker 5:\ttrial 1, jstack_rf => 97.0\n",
      "      From worker 3:\ttrial 3, crt_rpart => 27.0\n",
      "      From worker 3:\ttrial 1, jbest_ens => 97.0\n",
      "      From worker 3:\ttrial 1, skl_extratree => 97.0\n",
      "      From worker 2:\ttrial 1, jrf => 97.0\n",
      "      From worker 2:\ttrial 2, jbest_ens => 93.0\n",
      "      From worker 2:\ttrial 2, skl_extratree => 93.0\n",
      "      From worker 2:\ttrial 3, jbest_ens => 100.0\n",
      "      From worker 2:\ttrial 3, skl_extratree => 100.0\n",
      "      From worker 8:\ttrial 2, jsuper_ens => 93.0\n",
      "      From worker 8:\ttrial 2, sk_rf => 93.0\n",
      "      From worker 8:\ttrial 3, jsuper_ens => 100.0\n",
      "      From worker 8:\ttrial 3, sk_rf => 100.0\n",
      "      From worker 8:\ttrial 1, jsuper_ens => 97.0\n",
      "      From worker 8:\ttrial 1, sk_rf => 97.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>model</th><th>mean</th><th>std</th><th>n</th></tr><tr><th></th><th>Any</th><th>Float64</th><th>Float64</th><th>Int64</th></tr></thead><tbody><p>15 rows × 4 columns</p><tr><th>1</th><td>jada</td><td>97.7778</td><td>3.849</td><td>3</td></tr><tr><th>2</th><td>jrf</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>3</th><td>jbest_ens</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>4</th><td>skl_extratree</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>5</th><td>crt_treebag</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>6</th><td>skl_gb</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>7</th><td>crt_svmlinear</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>8</th><td>jsuper_ens</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>9</th><td>sk_rf</td><td>96.6667</td><td>3.33333</td><td>3</td></tr><tr><th>10</th><td>jstack_ada</td><td>95.5556</td><td>1.9245</td><td>3</td></tr><tr><th>11</th><td>crt_rf</td><td>95.5556</td><td>1.9245</td><td>3</td></tr><tr><th>12</th><td>jstack_rf</td><td>95.5556</td><td>1.9245</td><td>3</td></tr><tr><th>13</th><td>jvote_ens</td><td>95.5556</td><td>1.9245</td><td>3</td></tr><tr><th>14</th><td>skl_knn</td><td>94.4444</td><td>3.849</td><td>3</td></tr><tr><th>15</th><td>crt_rpart</td><td>28.8889</td><td>1.9245</td><td>3</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& model & mean & std & n\\\\\n",
       "\t\\hline\n",
       "\t& Any & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & jada & 97.7778 & 3.849 & 3 \\\\\n",
       "\t2 & jrf & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t3 & jbest\\_ens & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t4 & skl\\_extratree & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t5 & crt\\_treebag & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t6 & skl\\_gb & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t7 & crt\\_svmlinear & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t8 & jsuper\\_ens & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t9 & sk\\_rf & 96.6667 & 3.33333 & 3 \\\\\n",
       "\t10 & jstack\\_ada & 95.5556 & 1.9245 & 3 \\\\\n",
       "\t11 & crt\\_rf & 95.5556 & 1.9245 & 3 \\\\\n",
       "\t12 & jstack\\_rf & 95.5556 & 1.9245 & 3 \\\\\n",
       "\t13 & jvote\\_ens & 95.5556 & 1.9245 & 3 \\\\\n",
       "\t14 & skl\\_knn & 94.4444 & 3.849 & 3 \\\\\n",
       "\t15 & crt\\_rpart & 28.8889 & 1.9245 & 3 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "15×4 DataFrame\n",
       "│ Row │ model         │ mean    │ std     │ n     │\n",
       "│     │ \u001b[90mAny\u001b[39m           │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m │\n",
       "├─────┼───────────────┼─────────┼─────────┼───────┤\n",
       "│ 1   │ jada          │ 97.7778 │ 3.849   │ 3     │\n",
       "│ 2   │ jrf           │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 3   │ jbest_ens     │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 4   │ skl_extratree │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 5   │ crt_treebag   │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 6   │ skl_gb        │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 7   │ crt_svmlinear │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 8   │ jsuper_ens    │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 9   │ sk_rf         │ 96.6667 │ 3.33333 │ 3     │\n",
       "│ 10  │ jstack_ada    │ 95.5556 │ 1.9245  │ 3     │\n",
       "│ 11  │ crt_rf        │ 95.5556 │ 1.9245  │ 3     │\n",
       "│ 12  │ jstack_rf     │ 95.5556 │ 1.9245  │ 3     │\n",
       "│ 13  │ jvote_ens     │ 95.5556 │ 1.9245  │ 3     │\n",
       "│ 14  │ skl_knn       │ 94.4444 │ 3.849   │ 3     │\n",
       "│ 15  │ crt_rpart     │ 28.8889 │ 1.9245  │ 3     │"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = parallelmodel(learners,iris;trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a learner, evaluate, and view its workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.33333333333333"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_ind, test_ind) = holdout(size(iris, 1), 0.20)\n",
    "res,workflow=predict(learners[:jstack_rf],iris,train_ind,test_ind)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline\n",
      "Array{Transformer,1}\n",
      "├─ OneHotEncoder(Dict{Symbol,Any}(:nominal_column_values_map=>Dict{Int64,Any}(),:nominal_columns=>Int64[]), Dict(:nominal_column_values_map=>nothing,:nominal_columns=>nothing))\n",
      "├─ Imputer(Dict(:strategy=>mean), Dict(:strategy=>mean))\n",
      "├─ StandardScaler(Dict(:standardize_transform=>Standardize(4, [5.835, 3.085, 3.66, 1.14167], [1.15223, 2.27993, 0.551686, 1.29855])), Dict(:scale=>1,:center=>1))\n",
      "└─ StackEnsemble(Dict{Symbol,Any}(:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.85\n",
      "   Avg Depth:  2.8066666666666666, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 5\n",
      "   Depth:  4, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   84 samples\n",
      "    4 predictor\n",
      "    3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      10\n",
      "   Avg Leaves: 3.0\n",
      "   Avg Depth:  2.0, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>10,:partial_sampling=>0.7))),:label_map=>LabelMap (with 3 labels):\n",
      "   [1] setosa\n",
      "   [2] versicolor\n",
      "   [3] virginica\n",
      "   ), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.85\n",
      "   Avg Depth:  2.8066666666666666, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 5\n",
      "   Depth:  4, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   84 samples\n",
      "    4 predictor\n",
      "    3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      10\n",
      "   Avg Leaves: 3.0\n",
      "   Avg Depth:  2.0, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>10,:partial_sampling=>0.7))),:stacker_training_proportion=>0.3))\n"
     ]
    }
   ],
   "source": [
    "showtree(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick another learner, evaluate, and view its workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66666666666667"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_ind, test_ind) = holdout(size(iris, 1), 0.20)\n",
    "res,workflow=predict(learners[:jstack_ada],iris,train_ind,test_ind)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline\n",
      "Array{Transformer,1}\n",
      "├─ OneHotEncoder(Dict{Symbol,Any}(:nominal_column_values_map=>Dict{Int64,Any}(),:nominal_columns=>Int64[]), Dict(:nominal_column_values_map=>nothing,:nominal_columns=>nothing))\n",
      "├─ Imputer(Dict(:strategy=>mean), Dict(:strategy=>mean))\n",
      "├─ StandardScaler(Dict(:standardize_transform=>Standardize(4, [5.81583, 3.03917, 3.75167, 1.19083], [1.23485, 2.24334, 0.582214, 1.34074])), Dict(:scale=>1,:center=>1))\n",
      "└─ StackEnsemble(Dict{Symbol,Any}(:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 4.443333333333333\n",
      "   Avg Depth:  3.283333333333333, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 5\n",
      "   Depth:  4, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   84 samples\n",
      "    4 predictor\n",
      "    3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>Adaboost(Dict{Symbol,Any}(:ensemble=>Ensemble of Decision Trees\n",
      "   Trees:      7\n",
      "   Avg Leaves: 2.0\n",
      "   Avg Depth:  1.0,:coefficients=>[0.285272, 0.281884, 0.308174, 0.298515, 0.26341, 0.343184, 0.286657]), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict(:num_iterations=>7))),:label_map=>LabelMap (with 3 labels):\n",
      "   [1] versicolor\n",
      "   [2] virginica\n",
      "   [3] setosa\n",
      "   ), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 4.443333333333333\n",
      "   Avg Depth:  3.283333333333333, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 5\n",
      "   Depth:  4, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   84 samples\n",
      "    4 predictor\n",
      "    3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>Adaboost(Dict{Symbol,Any}(:ensemble=>Ensemble of Decision Trees\n",
      "   Trees:      7\n",
      "   Avg Leaves: 2.0\n",
      "   Avg Depth:  1.0,:coefficients=>[0.285272, 0.281884, 0.308174, 0.298515, 0.26341, 0.343184, 0.286657]), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict(:num_iterations=>7))),:stacker_training_proportion=>0.3))\n"
     ]
    }
   ],
   "source": [
    "showtree(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the super ensemble learner and view its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_ind, test_ind) = holdout(size(iris, 1), 0.20)\n",
    "res,workflow=predict(learners[:jsuper_ens],iris,train_ind,test_ind)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline\n",
      "Array{Transformer,1}\n",
      "├─ OneHotEncoder(Dict{Symbol,Any}(:nominal_column_values_map=>Dict{Int64,Any}(),:nominal_columns=>Int64[]), Dict(:nominal_column_values_map=>nothing,:nominal_columns=>nothing))\n",
      "├─ Imputer(Dict(:strategy=>mean), Dict(:strategy=>mean))\n",
      "├─ StandardScaler(Dict(:standardize_transform=>Standardize(4, [5.87333, 3.06083, 3.83, 1.24667], [1.24922, 2.39732, 0.568168, 1.2983])), Dict(:scale=>1,:center=>1))\n",
      "└─ VoteEnsemble(Dict(:learners=>[VoteEnsemble(Dict(:learners=>[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))]), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))])), StackEnsemble(Dict{Symbol,Any}(:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>Adaboost(Dict{Symbol,Any}(:ensemble=>Ensemble of Decision Trees\n",
      "   Trees:      7\n",
      "   Avg Leaves: 2.0\n",
      "   Avg Depth:  1.0,:coefficients=>[0.285272, 0.332118, 0.321616, 0.278327, 0.396149, 0.27051, 0.349147]), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict(:num_iterations=>7))),:label_map=>LabelMap (with 3 labels):\n",
      "   [1] virginica\n",
      "   [2] versicolor\n",
      "   [3] setosa\n",
      "   ), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>Adaboost(Dict{Symbol,Any}(:ensemble=>Ensemble of Decision Trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Trees:      7\n",
      "   Avg Leaves: 2.0\n",
      "   Avg Depth:  1.0,:coefficients=>[0.285272, 0.332118, 0.321616, 0.278327, 0.396149, 0.27051, 0.349147]), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict(:num_iterations=>7))),:stacker_training_proportion=>0.3)), StackEnsemble(Dict{Symbol,Any}(:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      10\n",
      "   Avg Leaves: 3.7\n",
      "   Avg Depth:  2.6, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>10,:partial_sampling=>0.7))),:label_map=>LabelMap (with 3 labels):\n",
      "   [1] virginica\n",
      "   [2] versicolor\n",
      "   [3] setosa\n",
      "   ), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      10\n",
      "   Avg Leaves: 3.7\n",
      "   Avg Depth:  2.6, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>10,:partial_sampling=>0.7))),:stacker_training_proportion=>0.3)), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))]), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[VoteEnsemble(Dict(:learners=>[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))]), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))])), StackEnsemble(Dict{Symbol,Any}(:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>Adaboost(Dict{Symbol,Any}(:ensemble=>Ensemble of Decision Trees\n",
      "   Trees:      7\n",
      "   Avg Leaves: 2.0\n",
      "   Avg Depth:  1.0,:coefficients=>[0.285272, 0.332118, 0.321616, 0.278327, 0.396149, 0.27051, 0.349147]), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict(:num_iterations=>7))),:label_map=>LabelMap (with 3 labels):\n",
      "   [1] virginica\n",
      "   [2] versicolor\n",
      "   [3] setosa\n",
      "   ), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>Adaboost(Dict{Symbol,Any}(:ensemble=>Ensemble of Decision Trees\n",
      "   Trees:      7\n",
      "   Avg Leaves: 2.0\n",
      "   Avg Depth:  1.0,:coefficients=>[0.285272, 0.332118, 0.321616, 0.278327, 0.396149, 0.27051, 0.349147]), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict(:num_iterations=>7))),:stacker_training_proportion=>0.3)), StackEnsemble(Dict{Symbol,Any}(:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      10\n",
      "   Avg Leaves: 3.7\n",
      "   Avg Depth:  2.6, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>10,:partial_sampling=>0.7))),:label_map=>LabelMap (with 3 labels):\n",
      "   [1] virginica\n",
      "   [2] versicolor\n",
      "   [3] setosa\n",
      "   ), Dict{Symbol,Any}(:output=>:class,:learners=>TSLearner[RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      300\n",
      "   Avg Leaves: 3.34\n",
      "   Avg Depth:  2.316666666666667, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>300,:partial_sampling=>0.7))), PrunedTree(Decision Tree\n",
      "   Leaves: 4\n",
      "   Depth:  3, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:purity_threshold=>1.0,:min_samples_split=>2,:min_samples_leaf=>1,:min_purity_increase=>0.0,:max_depth=>-1))), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), SKLearner(PyObject ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                        min_samples_leaf=1, min_samples_split=2,\n",
      "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                        oob_score=False, random_state=None, verbose=0,\n",
      "                        warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"ExtraTreesClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))],:keep_original_features=>false,:stacker=>RandomForest(Ensemble of Decision Trees\n",
      "   Trees:      10\n",
      "   Avg Leaves: 3.7\n",
      "   Avg Depth:  2.6, Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Symbol,Real}(:max_depth=>-1,:num_subfeatures=>0,:num_trees=>10,:partial_sampling=>0.7))),:stacker_training_proportion=>0.3)), SKLearner(PyObject GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                              max_features=None, max_leaf_nodes=None,\n",
      "                              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                              min_samples_leaf=1, min_samples_split=2,\n",
      "                              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                              n_iter_no_change=None, presort='auto',\n",
      "                              random_state=None, subsample=1.0, tol=0.0001,\n",
      "                              validation_fraction=0.1, verbose=0,\n",
      "                              warm_start=False), Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(:n_estimators=>10),:learner=>\"GradientBoostingClassifier\")), CaretLearner(RObject{VecSxp}\n",
      "   Random Forest \n",
      "   \n",
      "   120 samples\n",
      "     4 predictor\n",
      "     3 classes: 'setosa', 'versicolor', 'virginica' \n",
      "   \n",
      "   No pre-processing\n",
      "   Resampling: None \n",
      "   , Dict{Symbol,Any}(:output=>:class,:impl_args=>Dict{Any,Any}(),:learner=>\"rf\",:fitControl=>\"trainControl(method = 'none')\"))]))\n"
     ]
    }
   ],
   "source": [
    "showtree(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
